---
title: "Regression Analysis"
output: html_notebook
---

In this project, we will be using **Regression backward selection** for our analysis. The label for our data will be the rating.

```{r}
## first the imports
library(tidyverse)
library(broom)
library(GGally)
```

We need to first check to see if there's actually a linear relationship between the feature and labels.

```{r}
performance <- read_csv('../data/performance.csv')
glimpse(performance)
## using ggpairs to check the relationships
ggpairs(performance)
```

And we do actually see some linearity in the plots with overall_rating.

We will use the t-statistic to decide which variables are not as useful for our model and can be discarded. We will keep on this process until we have a t-statistic of \< 1.

```{r}
## for our linear model
## we have to first pass in the variable
## that we'd like our model to explain
## and then use . trick to have it use
## the rest of the columns as the input features
model <- lm(overall_rating ~ .,
            data = performance)
## and we can look at the summary of our model
summary(model)
```

Given the above summary, at the first glance we can say that given the high t-values and low probability for "handles_employee_complaints" and "space_for_learning", these two features are the most useful ones in our model. Considering the current values, the first feature that we will remove is the "not_overly_critical".

```{r}
## for our new model
## we can simply update the previous one
## the following way
## by using - not_overly_critical 
## from the right side
model_1 <- update(model,
                  . ~ . - not_overly_critical)
summary(model_1)
```

Given the updated model, now the next feature that could be removed is the "raises_for_performance", where we have the lowest t-value.

```{r}
## so we will be uodating our model once again
model_2 <- update(model_1,
                  . ~ . - raises_for_performance)
summary(model_2)
```

The next feature that can be removed is the "no_special_privileges", which has the lowest absolute t-value in our remaining features.

```{r}
model_3 <- update(model_2,
                  . ~ . - no_special_privileges)
summary(model_3)
```

Now, all of absolute t-values are above 1, and we can stop removing features at this point. The probability of getting the same results even after removing "rate_of_advancement" is still very high, but in order to avoid over-fitting, we will keep that feature for any future data that we might want to analyze.

We will move forward to create our residual plot, using the broom package.

```{r}
## first creating an augmented set
## using our last model
## which will contain the predictions
## as well as the residual values
performance_aug <- augment(model_3)
glimpse(performance_aug)
```

And then plotting our predictions as well as the residuals to make sure there's no specific pattern in the distribution of the residuals, which will show that the linear model is working as expected.

```{r}
## the residual plot using ggplot scatter
ggplot(performance_aug, aes(x = .fitted, y = .resid, color=.resid)) + 
  geom_point() +
  labs(x='Prediction', y='Residual') +
  geom_hline(yintercept = 0)
```

The last step is to compare the AIC for our models, and it's a better measure compared to $r^2$, primarily, because it gets worse by adding additional features that don't improve the model and are mainly noise, as opposed to the $r^2$, which improves with additional features, regardless of their true value.

```{r}
## first making a listof all models
model_list <- list(model, model_1,
                   model_2, model_3)
## and then apply the AIC
lapply(model_list, extractAIC)
```

As we can see, the AIC is going down with each iteration, which shows improvement in the performance of our model, even though we're removing data.
